# üìä Big Data Technologies (BDT)

This repository contains my work, skills, and learning outcomes from the **Big Data Technologies** module at Taylor‚Äôs University.  
The course provided a strong foundation in **distributed systems, Hadoop ecosystem, cloud Big Data tools, and Linux-based data processing**.

---

## üöÄ What I Learned

### üßµ Big Data Concepts
- Distributed file systems (HDFS, Replication, Fault Tolerance)
- Batch vs Real-Time Processing
- Data ingestion, ETL/ELT workflows
- NoSQL data models (HBase column families)
- Large-scale analytics engines (Spark, Hive, Impala)

### üõ† Technologies Used
- **Hadoop Ecosystem:** HDFS, MapReduce, Hive, Impala, HBase  
- **Apache Spark** (RDD, DataFrames, distributed processing)
- **Cloudera QuickStart VM** on VirtualBox
- **Microsoft Azure Big Data Tools:**  
  Event Hubs, IoT Hub, Data Factory, Data Lake, Synapse, HDInsight, Stream Analytics, Databricks  
- **Linux Mint VM** (Terminal + Bash)

### üêß Linux Commands Practiced
- Navigation: `ls`, `pwd`, `cd`
- File management: `mkdir`, `cp`, `rm`
- System info: `df`, `cat`, `ps`
- Hadoop commands:  
  - `hdfs dfs -ls`  
  - `hdfs dfs -put`  
  - `hdfs dfs -get`  
  - `hdfs dfs -cat`  
  - `hdfs dfs -mkdir`

### ‚òÅÔ∏è Cloud Architecture (Azure)
From my individual assignment:
- Event streaming: **Event Hubs & IoT Hub**
- Orchestration: **Azure Data Factory**
- Storage: **ADLS, Blob Storage**
- Processing: **Synapse (SQL Pool, Spark Pool)**  
- Streaming: **Azure Stream Analytics**
- Big Data clusters: **HDInsight (Hadoop, Spark, Hive)**  
- ML/ETL notebooks: **Azure Databricks**

---

## üß† Key Skills Gained
- Distributed Systems  
- Big Data Engineering  
- Hadoop & Spark ecosystem  
- ETL Pipelines & Data Warehousing  
- Cloud Analytics (Azure)  
- Linux & Virtualization  
- SQL & NoSQL processing  
- Hands-on cluster configuration (Cloudera)

